{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中英文分词及实现\n",
    "在语言理解中，词是最小的能独立活动且有意义的‘颗粒’。由词到句，由句成文。\n",
    "根据词性，标点符号对进行词语切分，将词确定下来是进行自然语言处理的第一步。\n",
    "\n",
    "> **示例：**\n",
    ">\n",
    ">  英文原文： i have a website,it is blog.oceaneyes.cn!\n",
    ">\n",
    ">  英文分词结果：i , have , a , website , it , is , blog.oceaneyes.cn, !\n",
    ">\n",
    ">  中文原文：  我有一个微信公众号，名字叫乔克叔叔杂货店！\n",
    ">\n",
    ">  中文分词结果：我/有/一个/微信公众号/，/名字/叫/乔克叔叔杂货店/！\n",
    "\n",
    "**本文简要目录**\n",
    "- 英文分词\n",
    "- 中文分词\n",
    "- 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 英文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于普通的英文文本不需要算法支撑，直接暴力拆分即可，可通过空格、标点符号将文本分开完成完成分词，可以使用.split()方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'have', 'a', 'website,it', 'is', 'blog.oceaneyes.cn!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用.split()方法，按照空格切分无限次\n",
    "a = 'i have a website,it is blog.oceaneyes.cn!'\n",
    "a1 =  a.split(' ')\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'have a website,it is blog.oceaneyes.cn!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用.split()方法，按照空格切分1次\n",
    "a2 = a.split(' ',1)\n",
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i ', ' a website,it is blog.oceaneyes.cn!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用.split()方法，按照have切分1次\n",
    "a3 = a.split('have',1)\n",
    "a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_english_text(text):\n",
    "    tokenize_text = []\n",
    "    for data in text:\n",
    "        tokenized_data = []       \n",
    "        for text0 in data.split('.'):  \n",
    "            print('text0是：',text0)\n",
    "            for text1 in text0.split('?'):  \n",
    "                print('text1是：',text1)\n",
    "                for text2 in text1.split('!'):\n",
    "                    print('text2是：',text2)\n",
    "                    for text3 in text2.split(','):\n",
    "                        print('text3是：',text3)\n",
    "                        for text4 in text3.split(' '):\n",
    "                            print('text4是：',text4)\n",
    "                            if text4 != '':\n",
    "                                tokenized_data.extend(text4)\n",
    "        \n",
    "        tokenize_text.append(tokenized_data)                         \n",
    "    return tokenize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text0是： i am a boy?i am a boy ! i am a boy,i\n",
      "text1是： i am a boy\n",
      "text2是： i am a boy\n",
      "text3是： i am a boy\n",
      "text4是： i\n",
      "text4是： am\n",
      "text4是： a\n",
      "text4是： boy\n",
      "text1是： i am a boy ! i am a boy,i\n",
      "text2是： i am a boy \n",
      "text3是： i am a boy \n",
      "text4是： i\n",
      "text4是： am\n",
      "text4是： a\n",
      "text4是： boy\n",
      "text4是： \n",
      "text2是：  i am a boy,i\n",
      "text3是：  i am a boy\n",
      "text4是： \n",
      "text4是： i\n",
      "text4是： am\n",
      "text4是： a\n",
      "text4是： boy\n",
      "text3是： i\n",
      "text4是： i\n",
      "text0是： god is a girl\n",
      "text1是： god is a girl\n",
      "text2是： god is a girl\n",
      "text3是： god is a girl\n",
      "text4是： god\n",
      "text4是： is\n",
      "text4是： a\n",
      "text4是： girl\n",
      "text0是： i love you!\n",
      "text1是： i love you!\n",
      "text2是： i love you\n",
      "text3是： i love you\n",
      "text4是： i\n",
      "text4是： love\n",
      "text4是： you\n",
      "text2是： \n",
      "text3是： \n",
      "text4是： \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['i',\n",
       "  'a',\n",
       "  'm',\n",
       "  'a',\n",
       "  'b',\n",
       "  'o',\n",
       "  'y',\n",
       "  'i',\n",
       "  'a',\n",
       "  'm',\n",
       "  'a',\n",
       "  'b',\n",
       "  'o',\n",
       "  'y',\n",
       "  'i',\n",
       "  'a',\n",
       "  'm',\n",
       "  'a',\n",
       "  'b',\n",
       "  'o',\n",
       "  'y',\n",
       "  'i'],\n",
       " ['g', 'o', 'd', 'i', 's', 'a', 'g', 'i', 'r', 'l'],\n",
       " ['i', 'l', 'o', 'v', 'e', 'y', 'o', 'u']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = ['i am a boy?i am a boy ! i am a boy,i', 'god is a girl', 'i love you!']\n",
    "tokenize_english_text(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在中文中，文本是连续的字序列组成，词与词之间没有天然的分隔符，且通常情况下，中文的一词多意。不同分词方法的结果都会影响词性、句法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **困难一：歧义**\n",
    ">\n",
    "> 原文：以前喜欢一个人，现在喜欢一个人\n",
    ">\n",
    "> 此处的有两个「一个人」，但单标的意思完全不同\n",
    ">\n",
    "> **困难二：分词界限**\n",
    ">\n",
    "> 原文：这杯水还没有冷\n",
    ">\n",
    "> 分词一： 这 / 杯 / 水 / 还 / 没有 / 冷\n",
    ">\n",
    "> 分词二： 这 / 杯 / 水 / 还没 / 有 / 冷\n",
    ">\n",
    ">分词三： 这 / 杯 / 水 / 还没有 / 冷\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 中文分词方法\n",
    "- 机械分词方法（又称为基于规则的分词方法）\n",
    "\n",
    "> 按照一定的规则将待处理的字符串与一个词表词典中的词进行逐一匹配\n",
    ">\n",
    "> 1、若在词典中找到某个字符串，则切分\n",
    ">2、若找不到，则不切分\n",
    "\n",
    "- 统计分词方法\n",
    "- 机械分词+统计分词合用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 机械分词方法\n",
    "- 正向最大匹配法\n",
    "\n",
    "      正向最大匹配法（Maximum Match Method，MM 法）是指从左向右按最大原则与词典里面的词进行匹配。假设词典中最长词是 m 个字，那么从待切分文本的最左边取 m个字符与词典进行匹配，如果匹配成功，则分词。如果匹配不成功，那么取 m−1 个字符与词典匹配，一直取直到成功匹配为止。\n",
    "      \n",
    "   -  **示例**\n",
    "    \n",
    "        句子： 中华民族从此站起来了\n",
    "        \n",
    "        词典：\"中华\"，\"民族\"，\"从此\"，\"站起来了\"\n",
    "        \n",
    "    \n",
    "   - **使用 MM 法分词**\n",
    "    \n",
    "        第一步：词典中最长是 4 个字，所以我们将 “中华民族” 取出来与词典进行匹配，匹配失败。\n",
    "        \n",
    "        第二步：于是，去掉 “族”，以 “中华民” 进行匹配，匹配失败\n",
    "        \n",
    "        第三步：去掉 “中华民” 中的 “民”，以 “中华” 进行匹配，匹配成功。\n",
    "        \n",
    "        第四步：在带切分句子中去掉匹配成功的词，待切分句子变成 “民族从此站起来了”。\n",
    "        \n",
    "        第五步：重复上面的第 1 - 4 步骤\n",
    "        \n",
    "        第六步：若最后一个词语匹配成功，结束。\n",
    "        \n",
    "        最终句子被分成：“中华 / 民族 / 从此 / 站起来了 ”\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 逆向最大匹配法\n",
    "\n",
    "      逆向最大匹配法（ Reverse Maximum Match Method, RMM 法）的原理与正向法基本相同，唯一不同的就是切分的方向与 M法相反。逆向法从文本末端开始匹配，每次用末端的最长词长度个字符进行匹配。\n",
    "\n",
    "\n",
    "- 双向匹配法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
